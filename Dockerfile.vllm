FROM python:3.11-slim

ENV PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1 \
    UV_SYSTEM_PYTHON=1

RUN apt-get update && apt-get install -y --no-install-recommends \
    git build-essential curl && \
    rm -rf /var/lib/apt/lists/*

RUN curl -LsSf https://astral.sh/uv/install.sh | sh
ENV PATH="/root/.local/bin:${PATH}"

WORKDIR /app

# Install pip and wheel tooling
RUN python -m pip install --upgrade pip setuptools wheel

# Install vLLM manually via pip. Install the OpenAI-compatible extras.
# Use --torch-backend=auto to pick appropriate torch variant (cpu/gpu) at install time.
RUN uv pip install --no-cache-dir "vllm[openai]" --torch-backend=auto

# Expose the OpenAI-compatible API port
EXPOSE 8000

# Default model and options (can be overridden via docker-compose env)
# Use Mistral 7B instruct model by default
ENV MODEL_NAME=mistralai/mistral-7b-instruct

# Start the vLLM OpenAI-compatible server
CMD ["python3", "-m", "vllm.entrypoints.openai.api_server", "--model", "${MODEL_NAME}", "--port", "8000", "--host", "0.0.0.0"]
